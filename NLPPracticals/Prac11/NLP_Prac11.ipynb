{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Prac11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vPLU5L9kbPE",
        "outputId": "85d55889-281a-402e-850c-39e9eb8ad69e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['Good', 'cake', 'cost', 'Rs.1500\\\\kg', 'in', 'Mumbai', '.']\n",
            "['Please', 'buy', 'me', 'one', 'of', 'them', '.']\n",
            "['Thanks', '.']\n"
          ]
        }
      ],
      "source": [
        "#a) Multiword Expressions in NLP\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "s = '''Good cake cost Rs.1500\\kg in Mumbai. Please buy me one of them.\\n\\nThanks.'''\n",
        "mwe = MWETokenizer([('New', 'York'), ('Hong', 'Kong')], separator='_')\n",
        "for sent in sent_tokenize(s):\n",
        "  print(mwe.tokenize(word_tokenize(sent)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#b) Normalized Web Distance and Word Similarity\n",
        "!pip install textdistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScdevIoclVz3",
        "outputId": "f9d97f00-8e51-4d09-addb-5abdb0ca2970"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textdistance\n",
            "  Downloading textdistance-4.2.2-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: textdistance\n",
            "Successfully installed textdistance-4.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import textdistance # pip install textdistance\n",
        "# we will need scikit-learn>=0.21\n",
        "import sklearn #pip install sklearn\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "texts = [\n",
        "'Reliance supermarket', 'Reliance hypermarket', 'Reliance', 'Reliance', 'Reliance downtown', 'Relianc market',\n",
        "'Mumbai', 'Mumbai Hyper', 'Mumbai dxb', 'mumbai airport',\n",
        "'k.m trading', 'KM Trading', 'KM trade', 'K.M. Trading', 'KM.Trading'\n",
        "]"
      ],
      "metadata": {
        "id": "SOoehmkHmTh8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(text):\n",
        "  \"\"\" Keep only lower-cased text and numbers\"\"\"\n",
        "  return re.sub('[^a-z0-9]+', ' ', text.lower())"
      ],
      "metadata": {
        "id": "u7WNYiQBle5k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group_texts(texts, threshold=0.4):\n",
        "  \"\"\" Replace each text with the representative of its cluster\"\"\"\n",
        "  normalized_texts = np.array([normalize(text) for text in texts])\n",
        "  distances = 1 - np.array([[textdistance.jaro_winkler(one, another) for one in normalized_texts] for another in normalized_texts])\n",
        "  clustering = AgglomerativeClustering(distance_threshold=threshold, # this parameter needs to be tuned carefully \n",
        "                    affinity=\"precomputed\", linkage=\"complete\", n_clusters=None).fit(distances)\n",
        "  centers = dict()\n",
        "  for cluster_id in set(clustering.labels_):\n",
        "    index = clustering.labels_ == cluster_id\n",
        "    centrality = distances[:, index][index].sum(axis=1)\n",
        "    centers[cluster_id] = normalized_texts[index][centrality.argmin()]\n",
        "  return [centers[i] for i in clustering.labels_]"
      ],
      "metadata": {
        "id": "jG_GUNwelhF-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(group_texts(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80AEAGMVmA7k",
        "outputId": "e1859b1a-8264-4492-fb74-5617d20bede3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'mumbai', 'mumbai', 'mumbai', 'mumbai', 'km trading', 'km trading', 'km trading', 'km trading', 'km trading']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#c) Word Sense Disambiguation\n",
        "from nltk.corpus import wordnet as wn\n",
        "def get_first_sense(word, pos=None):\n",
        "  if pos:\n",
        "    synsets = wn.synsets(word,pos)\n",
        "  else:\n",
        "    synsets = wn.synsets(word)\n",
        "  return synsets[0]"
      ],
      "metadata": {
        "id": "8uIu26lUmvlO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "best_synset = get_first_sense('bank')\n",
        "print ('%s: %s' % (best_synset.name, best_synset.definition))\n",
        "best_synset = get_first_sense('set','n')\n",
        "print ('%s: %s' % (best_synset.name, best_synset.definition))\n",
        "best_synset = get_first_sense('set','v')\n",
        "print ('%s: %s' % (best_synset.name, best_synset.definition))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZP9FjI9m6nU",
        "outputId": "4b52b037-e992-41e3-90d2-8095893d993a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "<bound method Synset.name of Synset('bank.n.01')>: <bound method Synset.definition of Synset('bank.n.01')>\n",
            "<bound method Synset.name of Synset('set.n.01')>: <bound method Synset.definition of Synset('set.n.01')>\n",
            "<bound method Synset.name of Synset('put.v.01')>: <bound method Synset.definition of Synset('put.v.01')>\n"
          ]
        }
      ]
    }
  ]
}